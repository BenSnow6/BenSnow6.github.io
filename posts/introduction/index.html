<!doctype html><html><head><title>Depth estimation project review</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><meta property="og:title" content="Depth estimation project review"><meta property="og:description" content="Depth Estimation project in review"><meta property="og:type" content="article"><meta property="og:url" content="https://BenSnow6.github.io/posts/introduction/"><meta property="article:published_time" content="2022-06-27T08:06:25+06:00"><meta property="article:modified_time" content="2022-06-27T08:06:25+06:00"><meta name=description content="Depth Estimation project in review"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>Ben Snow's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a class=active href=/posts/introduction/ title=Introduction>Introduction</a></li><li><a href=/posts/restructuring/ title=Restructuring>Restructuring</a></li><li><a href=/posts/serverlessml/lab-modules/ title=ServerlessML-module0>ServerlessML-module0</a></li><li><i class="fas fa-plus-circle"></i><a href>configuration</a><ul><li><a href=/posts/serverlessml/ title="Serverless ML">Serverless ML</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://BenSnow6.github.io/posts/introduction/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/avatar_hub7979d5c0b641038ae587dc7f4e229bf_336111_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Ben Snow</h5><p>:date_full</p></div><div class=title><h1>Depth estimation project review</h1></div><div class=post-content id=post-content><h3 id=introduction>Introduction</h3><p>Today we&rsquo;re going to take a look at a computer vision project on depth estimation and see how we can improve it.
The goals of this investigation are twofold</p><ul><li>Highlight the good and bad practices used in the project</li><li>Apply MLOps techniques to serve a production model in the cloud</li></ul><p>A few of the techniques to be included are the following: Unit testing, CI/CD, cloud serving, monitoring, and model experimentation and evaluation.
If you wish to read the original Jupyter notebook from the project then feel free to read it here. (Link to be added after anonymity is included)</p><h3 id=project-proposal>Project proposal</h3><p>The goal of the original project was to create a dataset of RGB and depth image pairs from the popular video game Grand Theft Auto V, GTAV. This dataset would then be used to train a convolutional neural network to predict a depth image from an RGB image. Using the trained model, a dataset of real-life images was used to evaluate the model&rsquo;s ability to predict real-world depth after training on this synthetic video game data. I will upload the entire project proposal document for you to read at your leisure, but this is the gist of the project.</p><p><img src=images/colour_simple.png class=center>
<em>Figure 1: Example colour image from GTAV within the training dataset. A little close for liking&mldr;</em><div style=margin-top:3rem></div></p><p><img src=images/depth_simple.png class=center>
<em>Figure 2: Associated depth image from the same dataset.</em><div style=margin-top:3rem></div></p><h3 id=approach>Approach</h3><p>I will read through the project as it currently stands and will critique the project&rsquo;s organisation and structure. I&rsquo;ll then follow the data from collection to pre-processing to get a feel for the data pipeline, then look at how the model architectures were defined. After this, I&rsquo;ll see how training, validation, and testing loops worked and give them a good talking to. I&rsquo;ll see what analysis techniques were used to analyse the predictions from the trained models and how these were interpreted. After this analysis, I will propose a series of improvements that will be made to take the project to a production standard.</p><h3 id=project-structure>Project structure</h3><p>Put simply, the project structure is poor. This is somewhat to be expected from a group of three master&rsquo;s students working together on their own jupyter notebooks and computers. Examples of bad practices found are:</p><ul><li>Inconsistent folder and file naming conventions (or none at all)</li><li>Monolithic jupyter notebooks</li><li>Attempt at containerisation but not fully working</li><li>No full list of package requirements</li><li>Different filetypes stored in the same folders (models stored together with README and datasets)</li><li>Just check out the structure below&mldr;</li></ul><p><img src=images/project%20structure.png class=center>
<em>Figure 3: Typical organisation structure of a group of researchers working together on a data science project.</em><div style=margin-top:2rem></div></p><h3 id=data-acquisition>Data acquisition</h3><p>A synthetic dataset was collected from GTAV in three stages: Simple collection, Moderate collection, and Hard collection.
These are defined below:</p><p><img src=images/collection_definitions.png class=center>
<em>Figure 4: Defining the data collection methods used to create three datasets for training, validating, and testing.</em><div style=margin-top:3rem></div></p><p>Data for the project was stored on Microsoft&rsquo;s OneDrive in a zipped folder that needed to be downloaded and extracted before it could be used by another researcher.
Data was shared between the group members during experimentation and had to be loaded onto a local machine in order to train a model.
During the project, only the Simple and moderate datasets were collected. The moderate dataset is outlined below:</p><p><img src=images/moderate_data.png class=center>
<em>Figure 5: Moderate dataset overview collected from GTAV.</em><div style=margin-top:3rem></div></p><p>Points to improve:</p><ul><li>Data collected is inaccessible and inflexible</li><li>Data is not versioned</li><li>Metadata saved in external .csv file</li></ul><h3 id=data-preprocessing>Data preprocessing</h3><p>Since the Simple collection dataset is only small, we will focus on the work done with the Moderate dataset.
The structuring of this dataset is defined in a .csv file that lists the splitting of data by weather conditions.
A python dataset class was created to read the data from a local machine to a pytorch dataset. Unfortunately, the method of reading data in this class is inefficient and clumsy. It relies on the use of &ldquo;f-strings&rdquo; to read in files from different folders, many &ldquo;if-else&rdquo; statements, and hardcoded paths. There is little documentation of the code and no unit testing for individual components of the pipeline. Although this code is not clean, it works for the specific use case it was written for, which tends to be the case in research environments.</p><p>Some data preprocessing was performed on the dataset including some transposes and conversions to torch tensors from numpy arrays.</p><p>The Moderate dataset class can be seen below, beware of some code smells&mldr;</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ModerateDataset</span>(Dataset):

    <span style=color:#66d9ef>def</span> __init__(self, col_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>, depth_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>, transform<span style=color:#f92672>=</span>None, trans_on<span style=color:#f92672>=</span>False):
        self<span style=color:#f92672>.</span>path_names <span style=color:#f92672>=</span> {}
        <span style=color:#66d9ef>for</span> folder <span style=color:#f92672>in</span> folder_names:
            self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#34;{folder}&#34;</span>] <span style=color:#f92672>=</span> {}
        <span style=color:#66d9ef>for</span> folder <span style=color:#f92672>in</span> folder_names:
            self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>] <span style=color:#f92672>=</span> {}
            self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>] <span style=color:#f92672>=</span> {}
        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, num_files[<span style=color:#ae81ff>0</span>]):
            self<span style=color:#f92672>.</span>path_names[<span style=color:#e6db74>&#39;Sunny&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#34;{i}&#34;</span>] <span style=color:#f92672>=</span> {}
            self<span style=color:#f92672>.</span>path_names[<span style=color:#e6db74>&#39;Sunny&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#34;{i}&#34;</span>] <span style=color:#f92672>=</span> {}
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;*************MAKE SURE THE PATH FILE IN THE FOR LOOP IS THE BASE IMAGE DIRECTORY ON YOUR COMPUTER**************&#34;</span>)
        count <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
        <span style=color:#66d9ef>for</span> folder <span style=color:#f92672>in</span> folder_names:
            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_files[folder_names<span style=color:#f92672>.</span>index(folder)]):
                self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{i+1}&#39;</span>] <span style=color:#f92672>=</span> Path(f<span style=color:#e6db74>&#34;C:/Users/Admin/OneDrive/Computer Vision/Moderate collection/{folder}/colour/{colour_filenames[count+i]}&#34;</span>)  <span style=color:#75715e>## Change this path here!!!!</span>
                self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{i+1}&#39;</span>] <span style=color:#f92672>=</span> Path(f<span style=color:#e6db74>&#34;C:/Users/Admin/OneDrive/Computer Vision/Moderate collection/{folder}/depth/{depth_filenames[count+i]}&#34;</span>)   <span style=color:#75715e>## Change this path here!!!!</span>
            count <span style=color:#f92672>=</span> count <span style=color:#f92672>+</span> num_files[folder_names<span style=color:#f92672>.</span>index(folder)]
        
        self<span style=color:#f92672>.</span>transform <span style=color:#f92672>=</span> transform
        self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> col_dir
        self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> depth_dir
        self<span style=color:#f92672>.</span>trans_on <span style=color:#f92672>=</span> trans_on

    <span style=color:#66d9ef>def</span> __getitem__(self,idx):
        <span style=color:#66d9ef>if</span> idx <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
            
            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[0]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx+1}&#39;</span>]
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[0]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx+1}&#39;</span>]
        
        <span style=color:#66d9ef>if</span> (idx<span style=color:#f92672>&gt;</span><span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;=</span> num_files[<span style=color:#ae81ff>0</span>]):  <span style=color:#75715e>## 1-500</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[0]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx}&#39;</span>]
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[0]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx}&#39;</span>]

        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> num_files[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;</span> (sum(num_files[:<span style=color:#ae81ff>2</span>])<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)): <span style=color:#75715e>## 501 - 1500</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[1]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx-num_files[0]}&#39;</span>]
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[1]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx-num_files[0]}&#39;</span>]

        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> sum(num_files[:<span style=color:#ae81ff>2</span>]) <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;</span> (sum(num_files[:<span style=color:#ae81ff>3</span>])<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>) ): <span style=color:#75715e>## 1501 - 2600</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[2]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:2])}&#39;</span>] <span style=color:#75715e># -1500</span>
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[2]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:2])}&#39;</span>]

        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> sum(num_files[:<span style=color:#ae81ff>3</span>]) <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;</span> (sum(num_files[:<span style=color:#ae81ff>4</span>])<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>) ): <span style=color:#75715e>## 2601 - 5600</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[3]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:3])}&#39;</span>] <span style=color:#75715e>#-2600</span>
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[3]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:3])}&#39;</span>]
            
        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> sum(num_files[:<span style=color:#ae81ff>4</span>]) <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;</span> (sum(num_files[:<span style=color:#ae81ff>5</span>])<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>) ): <span style=color:#75715e>## 5601 - 7857</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[4]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:4])}&#39;</span>] <span style=color:#75715e># -5600</span>
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[4]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:4])}&#39;</span>]

        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> sum(num_files)):
            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>NameError</span>(<span style=color:#e6db74>&#39;Index outside of range&#39;</span>)

        col_img <span style=color:#f92672>=</span> import_raw_colour_image(self<span style=color:#f92672>.</span>col_dir)
        depth_img <span style=color:#f92672>=</span> import_raw_depth_image(self<span style=color:#f92672>.</span>depth_dir)
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>trans_on <span style=color:#f92672>==</span> True:
            col_img <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(np<span style=color:#f92672>.</span>flip(col_img,axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>copy()) <span style=color:#75715e># apply any transforms</span>
            depth_img <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(np<span style=color:#f92672>.</span>flip(depth_img,axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>copy()) <span style=color:#75715e># apply any transforms</span>
            col_img <span style=color:#f92672>=</span> col_img<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>2</span>)
            col_img <span style=color:#f92672>=</span> col_img<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>)
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>transform: <span style=color:#75715e># if any transforms were given to initialiser</span>
            col_img <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transform(col_img) <span style=color:#75715e># apply any transforms</span>
        <span style=color:#66d9ef>return</span> col_img, depth_img
    
    <span style=color:#66d9ef>def</span> __len__(self):
        <span style=color:#66d9ef>return</span> sum(num_files)
</code></pre></div><p>Creating an instance of this dataset class into the variable <code>total_data</code> allowed the splitting of data into three components.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>total_data <span style=color:#f92672>=</span> ModerateDataset(trans_on<span style=color:#f92672>=</span>True) <span style=color:#75715e>## Instantiating the dataset</span>
</code></pre></div><p>It was of importance to establish the separation of three datasets: training, validation, and testing. Training data was used to train the neural network model and validation data is used to check that the model was not overfit to the training data. Testing data was used to check the performance of the trained model on unseen data to evaluate performance with a set of predefined metrics, defined in a later section.</p><p>A train, validation and, testing split of 80/10/10 has been used to create three datasets: <code>train_dataset</code>, <code>val_dataset</code> and <code>test_dataset</code>. These datasets all inherit from the <code>ModerateDasaset</code> class.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_size <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> len(total_Data)) <span style=color:#75715e># Size of training dataset (80% of total)</span>
val_size <span style=color:#f92672>=</span> int((len(total_Data) <span style=color:#f92672>-</span> train_size)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>) <span style=color:#75715e>## Size of validation and test datasets (10% of total)</span>
train_dataset, val_dataset, test_dataset <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>random_split(total_Data, [train_size, val_size, val_size]) <span style=color:#75715e># train, val, and test splits</span>
</code></pre></div><p>For each of these datasets, a data loader was created to load a batch of images at once instead of loading the entire dataset to memory. To train the model, the training and validation dataloaders are used. This ensures that no testing data is used in any step of training the model.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>batch_sz <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span> <span style=color:#75715e># Batch size</span>
tr_dl  <span style=color:#f92672>=</span> DataLoader(train_dataset,  batch_size<span style=color:#f92672>=</span>batch_sz, shuffle<span style=color:#f92672>=</span>True,  num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># Training dataloader</span>
val_dl <span style=color:#f92672>=</span> DataLoader(val_dataset,  batch_size<span style=color:#f92672>=</span>batch_sz, shuffle<span style=color:#f92672>=</span>True,  num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)   <span style=color:#75715e># Validation dataloader</span>
test_dl <span style=color:#f92672>=</span> DataLoader(test_dataset,  batch_size<span style=color:#f92672>=</span>batch_sz, shuffle<span style=color:#f92672>=</span>True,  num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># Test dataloader</span>
</code></pre></div><p>Points to improve</p><ul><li>Inflexible dataset reading, can&rsquo;t add new data easily</li><li>None of the code is unit tested</li><li>Inconsistent naming conventions</li><li>Hard coded values and filepaths</li><li>If-Else statements aplenty</li></ul><h3 id=defining-the-model>Defining the model</h3><p>A simple CNN architecture was developed as a baseline model to compare performance against. Ideally, the team planned on using a more complex model to experiment with but time constraints made this unfeasable. The neural network is defined with pytorch to be the following:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
    nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,  out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>), 
    nn<span style=color:#f92672>.</span>ReLU(),
    nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>12</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
    nn<span style=color:#f92672>.</span>ReLU(),
    nn<span style=color:#f92672>.</span>ConvTranspose2d(in_channels <span style=color:#f92672>=</span> <span style=color:#ae81ff>12</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
    nn<span style=color:#f92672>.</span>ReLU(),
    nn<span style=color:#f92672>.</span>ConvTranspose2d(in_channels <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
    nn<span style=color:#f92672>.</span>Sigmoid()
)<span style=color:#f92672>.</span>cuda()
</code></pre></div><p>Using model summary, we can see the network&rsquo;s structure when given an input of shape (3,720,1280), this being the resolution of the collected colour images.</p><p><img src=images/simpleCNN.png class=center>
<em>Figure 6: A simple CNN architecture used to convert colour images to depth images.</em><div style=margin-top:3rem></div></p><p>After creating the pytorch model, a training loop was developed to train the network on the training dataset whilst validating with the validation dataset. A description of the hyperparameters used to train the model can be seen below.</p><p><img src=images/simplecnn_desc.png class=center>
<em>Figure 7: Simple CNN hyperparameters and tracked variables during training.</em><div style=margin-top:3rem></div></p><p>Points to improve</p><ul><li>Hyperparameter search</li><li>Experiment tracking</li><li>Different model architecture search</li></ul><p>The training loop itself can be seen below.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(net, tr_dl, val_dl, loss<span style=color:#f92672>=</span>nn<span style=color:#f92672>.</span>MSELoss(), epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>3e-3</span>, wd<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>):   

    Ltr_hist, Lval_hist <span style=color:#f92672>=</span> [], []    
    opt <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(net<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>lr, weight_decay<span style=color:#f92672>=</span>wd)
    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> trange(epochs):
        
        L <span style=color:#f92672>=</span> []
        dl <span style=color:#f92672>=</span> (iter(tr_dl))
        count_train <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
        <span style=color:#66d9ef>for</span> xb, yb <span style=color:#f92672>in</span> tqdm(dl, leave<span style=color:#f92672>=</span>False):
            xb, yb <span style=color:#f92672>=</span> xb<span style=color:#f92672>.</span>float(), yb<span style=color:#f92672>.</span>float()
            xb, yb <span style=color:#f92672>=</span> xb<span style=color:#f92672>.</span>cuda(), yb<span style=color:#f92672>.</span>cuda()
            y_ <span style=color:#f92672>=</span> net(xb)
            l <span style=color:#f92672>=</span> loss(y_, yb)
            opt<span style=color:#f92672>.</span>zero_grad()
            l<span style=color:#f92672>.</span>backward()
            opt<span style=color:#f92672>.</span>step()
            L<span style=color:#f92672>.</span>append(l<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy())
            <span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Training on batch {count_train} of {int(train_size/batch_sz)}&#34;</span>)
            count_train<span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>

        <span style=color:#75715e># disable gradient calculations for validation</span>
        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> net<span style=color:#f92672>.</span>parameters(): p<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> False

        Lval, Aval <span style=color:#f92672>=</span> [], []
        val_it <span style=color:#f92672>=</span> iter(val_dl)
        val_count <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
        <span style=color:#66d9ef>for</span> xb, yb <span style=color:#f92672>in</span> tqdm(val_it, leave<span style=color:#f92672>=</span>False):
            xb, yb <span style=color:#f92672>=</span> xb<span style=color:#f92672>.</span>float(), yb<span style=color:#f92672>.</span>float()
            xb, yb <span style=color:#f92672>=</span> xb<span style=color:#f92672>.</span>cuda(), yb<span style=color:#f92672>.</span>cuda()
            y_ <span style=color:#f92672>=</span> net(xb)
            l <span style=color:#f92672>=</span> loss(y_, yb)
            Lval<span style=color:#f92672>.</span>append(l<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy())
            Aval<span style=color:#f92672>.</span>append((y_<span style=color:#f92672>.</span>max(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>==</span> yb)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy())
            <span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Validating on batch {val_count} of {int(val_size/batch_sz)}&#34;</span>)
            val_count<span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>

        <span style=color:#75715e># enable gradient calculations for next epoch </span>
        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> net<span style=color:#f92672>.</span>parameters(): p<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> True 
            
        Ltr_hist<span style=color:#f92672>.</span>append(np<span style=color:#f92672>.</span>mean(L))
        Lval_hist<span style=color:#f92672>.</span>append(np<span style=color:#f92672>.</span>mean(Lval))
        <span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#39;training loss: {np.mean(L):0.4f}</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>validation loss: {np.mean(Lval):0.4f}</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>validation accuracy: {np.mean(Aval):0.2f}&#39;</span>)
    <span style=color:#66d9ef>return</span> Ltr_hist, Lval_hist
</code></pre></div><p>Just like the code seen before, there are quite a few improvements that can be made to bring the quality to a higher standard. Some of these include:</p><ul><li>Remove timing iterators</li><li>Naming conventions</li><li>Remove print functions</li><li>Tracking/ logging during training</li><li>Model versioning</li></ul><h3 id=model-evaluation>Model evaluation</h3><p>A trained SimpleCNN model was saved so that it could be reused for the evaluation stage at a later date. This worked for the project, but a more rigorous approach can be used in future. Linking the model to the data used to train it and the hyperparameters used during training for example. Along with this, storing the model in a repository so that other users can easily access it and understand where it came from and how it was trained.</p><p>During the evaluation stage, the validation dataset was used to predict depth images from the RGB images. A series of metrics were then calculated to compare the predicted depth images and the ground truth data. This process was then repeated for the test set data that the model was not trained with. In total, nine metrics were calculated for all 786 test images. For this testing dataset, the mean and standard deviation of these metrics were calculated. A list of these metrics for the validation and test sets can be seen below.</p><p><img src=images/val_test_errors.png class=center>
<em>Figure 8: Calculated metrics with their mean and standard deviations for the validation and test datasets.</em><div style=margin-top:3rem></div></p><p>After evaluation on the test set, some discussion of the metrics was made to understand the predictions made by the model. It would have been nice to see some more visual analysis, perhaps an ablation study to assess the features in different layers of the CNN. It would have also been nice to see some side-by-side comparisons of the depth image predictions and ground truths to look for artefacts where the model is lacking in accuracy. Since metrics were calculated in a pixel-wise fashion, it would be interesting to see them plotted as an image where each pixel represents the metric calculated at that point in the image. This would help give a visual representation of where in the image the error metrics are high and low along with any other high level features that may be occuring.</p><p>An external dataset, called the KITTI dataset, was also used for evaluation. A subset of the KITTI dataset used for evaluation consisted of 1000 RGB, depth image pairs taken from Lidar sensors attatched to a car that was driving down a suburban road.</p><p><img src=images/kittiRGB.png class=center>
<em>Figure 9: Sample RGB image from the KITTI dataset.</em><div style=margin-top:3rem></div></p><p><img src=images/kittidepth.png class=center>
<em>Figure 10: Associated depth image from KITTI dataset.</em><div style=margin-top:3rem></div></p><p><img src=images/simplekittidepth.png class=center>
<em>Figure 11: Predicted depth image from SimpleCNN model.</em><div style=margin-top:3rem></div></p><p>Additional explanation of the depth images generated from predictions of the SimpleCNN model would be nice. It would be interesting to explain the colours used in the plots and the min/max values of each depth image to better understand what has been calculated. A list of relationships required to do this are highlighted below.</p><p><img src=images/requiredData.png class=center>
<em>Figure 12: Additional relationships required to progress the evaluation of the SimpleCNN&rsquo;s predictions against the KITTI benchmark.</em><div style=margin-top:3rem></div></p><p>Error metrics were calculated with a mean and standard deviation allowing paving way for hypothesis testing upon changing model hyperparameters. Unfortunately, the error metrics calculated are missing some scaling factors to allow for direct comparison between the Moderate dataset depth predictions and the KITTI dataset depth. If these factors are found, this will allow a swath of analysis to be conducted. Conversions from the current error values to SI units would be a great improvement.</p><p>Error analysis can be improved by assessing the quality of the metrics themselves and interpreting their meaning in relation to the task. During my physics degree, I enjoyed this aspect of analysis. As such, in this revamp of the project I&rsquo;m keen to look more in depth at the error analysis and metrics.</p><p>Points to improve</p><ul><li>Model versioning</li><li>Metric evaluation</li><li>Error analysis</li><li>Depth conversion and comparison</li></ul><h3 id=summary>Summary</h3><p>Having looked through the project, there are many good techniques used and a decent overarching methodology presented. To take the project to another level it will be useful to flesh out the experimentation procedure and increase the reusability and scalability of the code used. Best practices involving unit testing, code versioning, continuous integration/ continuous deployment, and code documentation are to be used. Utilising these best practices will make the project more robust and scalable. Read the next post to see what the first steps of this process are.</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/BenSnow6/BenSnow6.github.io/edit/main/content/posts/Introduction/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/restructuring/ title="Restructuring a project" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Restructuring a project</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><ul><li><a href=#introduction>Introduction</a></li><li><a href=#project-proposal>Project proposal</a></li><li><a href=#approach>Approach</a></li><li><a href=#project-structure>Project structure</a></li><li><a href=#data-acquisition>Data acquisition</a></li><li><a href=#data-preprocessing>Data preprocessing</a></li><li><a href=#defining-the-model>Defining the model</a></li><li><a href=#model-evaluation>Model evaluation</a></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:bensnows@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span><span>bensnows@gmail.com</span></a></li><li><span><i class="fas fa-phone-alt"></i></span><span>+44 7794748089</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form method=post action=https://blogtrottr.com><div class=form-group><input type=email class=form-control name=btr_email placeholder="Enter email"><br><input type=hidden name=btr_url value=https://BenSnow6.github.ioindex.xml>
<input type=hidden name=schedule_type value=1>
<small id=emailHelp class="form-text text-muted">By entering your email address, you agree to receive the newsletter of this website.</small>
<button type=submit class="btn btn-info"> Submit</button></div></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>