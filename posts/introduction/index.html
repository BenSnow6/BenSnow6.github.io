<!doctype html><html><head><title>Depth estimation project review</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><meta property="og:title" content="Depth estimation project review"><meta property="og:description" content="Introduction to Sample Post"><meta property="og:type" content="article"><meta property="og:url" content="https://BenSnow6.github.io/posts/introduction/"><meta property="article:published_time" content="2022-06-17T08:06:25+06:00"><meta property="article:modified_time" content="2022-06-17T08:06:25+06:00"><meta name=description content="Introduction to Sample Post"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>Ben Snow's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a class=active href=/posts/introduction/ title=Introduction>Introduction</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://BenSnow6.github.io/posts/introduction/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/avatar_hub7979d5c0b641038ae587dc7f4e229bf_336111_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Ben Snow</h5><p>:date_full</p></div><div class=title><h1>Depth estimation project review</h1></div><div class=post-content id=post-content><h3 id=introduction>Introduction</h3><p>Today we&rsquo;re going to take a look at a computer vision project on depth estimation and see how we can improve it.
The goals of this investigation are twofold</p><ul><li>Highlight the good and bad practices used in the project</li><li>Apply MLOps techniques to serve a production model in the cloud</li></ul><p>A few of the techniques to be included are the following: Unit testing, CI/CD, cloud serving, monitoring, and model experimentation and evaluation.
If you wish to read the original Jupyter notebook from the project then feel free to read it here. (Link to be added after anonymity is included)</p><h3 id=project-proposal>Project proposal</h3><p>The goal of the original project was to create a dataset of RGBA and depth image pairs from the popular video game Grand Theft Auto V, GTAV. This dataset would then be used to train a convolutional neural network to predict a depth image from an RGBA image. Using the trained model, a dataset of real-life images was used to evaluate the model&rsquo;s ability to predict real-world depth after training on this synthetic video game data. I will upload the entire project proposal document for you to read at your leisure, but this is the gist of the project.</p><p><img src=images/colour_simple.png class=center>
<em>Figure 1: Example colour image from GTAV within the training dataset. A little close for liking&mldr;</em><div style=margin-top:3rem></div></p><p><img src=images/depth_simple.png class=center>
<em>Figure 2: Associated depth image from the same dataset.</em><div style=margin-top:3rem></div></p><h3 id=approach>Approach</h3><p>I will read through the project as it currently stands and will critique the project&rsquo;s organisation and structure. I&rsquo;ll then follow the data from collection to pre-processing to get a feel for the data pipeline, then look at how the model architectures were defined. After this, I&rsquo;ll see how training, validation, and testing loops worked and give them a good talking to. I&rsquo;ll see what analysis techniques were used to analyse the predictions from the trained models and how these were interpreted. After this analysis, I will propose a series of improvements that will be made to take the project to a production standard.</p><h3 id=project-structure>Project structure</h3><p>Put simply, the project structure is poor. This is somewhat to be expected from a group of three master&rsquo;s students working together on their own jupyter notebooks and computers. Examples of bad practices found are:</p><ul><li>Inconsistent folder and file naming conventions (or none at all)</li><li>Monolithic jupyter notebooks</li><li>Attempt at containerisation but not fully working</li><li>No full list of package requirements</li><li>Different filetypes stored in the same folders (models stored together with README and datasets)</li><li>Just check out the structure below&mldr;</li></ul><p><img src=images/project%20structure.png class=center>
<em>Figure 3: Typical organisation structure of a group of researchers working together on a data science project.</em><div style=margin-top:2rem></div></p><h3 id=data-acquisition>Data acquisition</h3><p>A synthetic dataset was collected from GTAV in three stages: Simple collection, Moderate collection, and Hard collection.
These are defined below:</p><p><img src=images/collection_definitions.png class=center>
<em>Figure 4: Defining the data collection methods used to create three datasets for training, validating, and testing.</em><div style=margin-top:3rem></div></p><p>Data for the project was stored on Microsoft&rsquo;s OneDrive in a zipped folder that needed to be downloaded and extracted before it could be used by another researcher.
Data was shared between the group members during experimentation and had to be loaded onto a local machine in order to train a model.
During the project, only the Simple and moderate datasets were collected. The moderate dataset is outlined below:</p><p><img src=images/moderate_data.png class=center>
<em>Figure 5: Moderate dataset overview collected from GTAV.</em><div style=margin-top:3rem></div></p><h3 id=data-preprocessing>Data preprocessing</h3><p>Since the Simple collection dataset is only small, we will focus on the work done with the Moderate dataset.
The structuring of this dataset is defined in a .csv file that lists the splitting of data by weather conditions.
A python dataset class was created to read the data from a local machine to a pytorch dataset. Unfortunately, the method of reading data in this class is inefficient and clumsy. It relies on the use of &ldquo;f-strings&rdquo; to read in files from different folders, many &ldquo;if-else&rdquo; statements, and hardcoded paths. There is little documentation of the code and no unit testing for individual components of the pipeline. Although this code is not clean, it works for the specific use case it was written for, which tends to be the case in research environments.</p><p>Some data preprocessing was performed on the dataset including some transposes and conversions to torch tensors from numpy arrays.</p><p>The Moderate dataset class can be seen below, beware of some code smells&mldr;</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ModerateDataset</span>(Dataset):

    <span style=color:#66d9ef>def</span> __init__(self, col_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>, depth_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>, transform<span style=color:#f92672>=</span>None, trans_on<span style=color:#f92672>=</span>False):
        self<span style=color:#f92672>.</span>path_names <span style=color:#f92672>=</span> {}
        <span style=color:#66d9ef>for</span> folder <span style=color:#f92672>in</span> folder_names:
            self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#34;{folder}&#34;</span>] <span style=color:#f92672>=</span> {}
        <span style=color:#66d9ef>for</span> folder <span style=color:#f92672>in</span> folder_names:
            self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>] <span style=color:#f92672>=</span> {}
            self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>] <span style=color:#f92672>=</span> {}
        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, num_files[<span style=color:#ae81ff>0</span>]):
            self<span style=color:#f92672>.</span>path_names[<span style=color:#e6db74>&#39;Sunny&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#34;{i}&#34;</span>] <span style=color:#f92672>=</span> {}
            self<span style=color:#f92672>.</span>path_names[<span style=color:#e6db74>&#39;Sunny&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#34;{i}&#34;</span>] <span style=color:#f92672>=</span> {}
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;*************MAKE SURE THE PATH FILE IN THE FOR LOOP IS THE BASE IMAGE DIRECTORY ON YOUR COMPUTER**************&#34;</span>)
        count <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
        <span style=color:#66d9ef>for</span> folder <span style=color:#f92672>in</span> folder_names:
            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_files[folder_names<span style=color:#f92672>.</span>index(folder)]):
                self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{i+1}&#39;</span>] <span style=color:#f92672>=</span> Path(f<span style=color:#e6db74>&#34;C:/Users/Admin/OneDrive/Computer Vision/Moderate collection/{folder}/colour/{colour_filenames[count+i]}&#34;</span>)  <span style=color:#75715e>## Change this path here!!!!</span>
                self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{i+1}&#39;</span>] <span style=color:#f92672>=</span> Path(f<span style=color:#e6db74>&#34;C:/Users/Admin/OneDrive/Computer Vision/Moderate collection/{folder}/depth/{depth_filenames[count+i]}&#34;</span>)   <span style=color:#75715e>## Change this path here!!!!</span>
            count <span style=color:#f92672>=</span> count <span style=color:#f92672>+</span> num_files[folder_names<span style=color:#f92672>.</span>index(folder)]
        
        self<span style=color:#f92672>.</span>transform <span style=color:#f92672>=</span> transform
        self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> col_dir
        self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> depth_dir
        self<span style=color:#f92672>.</span>trans_on <span style=color:#f92672>=</span> trans_on

    <span style=color:#66d9ef>def</span> __getitem__(self,idx):
        <span style=color:#66d9ef>if</span> idx <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
            
            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[0]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx+1}&#39;</span>]
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[0]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx+1}&#39;</span>]
        
        <span style=color:#66d9ef>if</span> (idx<span style=color:#f92672>&gt;</span><span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;=</span> num_files[<span style=color:#ae81ff>0</span>]):  <span style=color:#75715e>## 1-500</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[0]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx}&#39;</span>]
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[0]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx}&#39;</span>]

        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> num_files[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;</span> (sum(num_files[:<span style=color:#ae81ff>2</span>])<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)): <span style=color:#75715e>## 501 - 1500</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[1]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx-num_files[0]}&#39;</span>]
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[1]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx-num_files[0]}&#39;</span>]

        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> sum(num_files[:<span style=color:#ae81ff>2</span>]) <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;</span> (sum(num_files[:<span style=color:#ae81ff>3</span>])<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>) ): <span style=color:#75715e>## 1501 - 2600</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[2]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:2])}&#39;</span>] <span style=color:#75715e># -1500</span>
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[2]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:2])}&#39;</span>]

        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> sum(num_files[:<span style=color:#ae81ff>3</span>]) <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;</span> (sum(num_files[:<span style=color:#ae81ff>4</span>])<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>) ): <span style=color:#75715e>## 2601 - 5600</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[3]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:3])}&#39;</span>] <span style=color:#75715e>#-2600</span>
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[3]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:3])}&#39;</span>]
            
        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> sum(num_files[:<span style=color:#ae81ff>4</span>]) <span style=color:#f92672>and</span> idx <span style=color:#f92672>&lt;</span> (sum(num_files[:<span style=color:#ae81ff>5</span>])<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>) ): <span style=color:#75715e>## 5601 - 7857</span>

            self<span style=color:#f92672>.</span>col_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[4]}&#39;</span>][<span style=color:#e6db74>&#39;colour&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:4])}&#39;</span>] <span style=color:#75715e># -5600</span>
            self<span style=color:#f92672>.</span>depth_dir <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>path_names[f<span style=color:#e6db74>&#39;{folder_names[4]}&#39;</span>][<span style=color:#e6db74>&#39;depth&#39;</span>][f<span style=color:#e6db74>&#39;{idx-sum(num_files[:4])}&#39;</span>]

        <span style=color:#66d9ef>elif</span> (idx <span style=color:#f92672>&gt;</span> sum(num_files)):
            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>NameError</span>(<span style=color:#e6db74>&#39;Index outside of range&#39;</span>)

        col_img <span style=color:#f92672>=</span> import_raw_colour_image(self<span style=color:#f92672>.</span>col_dir)
        depth_img <span style=color:#f92672>=</span> import_raw_depth_image(self<span style=color:#f92672>.</span>depth_dir)
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>trans_on <span style=color:#f92672>==</span> True:
            col_img <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(np<span style=color:#f92672>.</span>flip(col_img,axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>copy()) <span style=color:#75715e># apply any transforms</span>
            depth_img <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(np<span style=color:#f92672>.</span>flip(depth_img,axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>copy()) <span style=color:#75715e># apply any transforms</span>
            col_img <span style=color:#f92672>=</span> col_img<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>2</span>)
            col_img <span style=color:#f92672>=</span> col_img<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>)
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>transform: <span style=color:#75715e># if any transforms were given to initialiser</span>
            col_img <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transform(col_img) <span style=color:#75715e># apply any transforms</span>
        <span style=color:#66d9ef>return</span> col_img, depth_img
    
    <span style=color:#66d9ef>def</span> __len__(self):
        <span style=color:#66d9ef>return</span> sum(num_files)
</code></pre></div><p>Creating an instance of this dataset class into the variable <code>total_data</code> allowed the splitting of data into three components.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>total_data <span style=color:#f92672>=</span> ModerateDataset(trans_on<span style=color:#f92672>=</span>True) <span style=color:#75715e>## Instantiating the dataset</span>
</code></pre></div><p>It was of importance to establish the separation of three datasets: training, validation, and testing. Training data was used to train the neural network model and validation data is used to check that the model was not overfit to the training data. Testing data was used to check the performance of the trained model on unseen data to evaluate performance with a set of predefined metrics, defined in a later section.</p><p>A train, validation and, testing split of 80/10/10 has been used to create three datasets: <code>train_dataset</code>, <code>val_dataset</code> and <code>test_dataset</code>. These datasets all inherit from the <code>ModerateDasaset</code> class.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_size <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> len(total_Data)) <span style=color:#75715e># Size of training dataset (80% of total)</span>
val_size <span style=color:#f92672>=</span> int((len(total_Data) <span style=color:#f92672>-</span> train_size)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>) <span style=color:#75715e>## Size of validation and test datasets (10% of total)</span>
train_dataset, val_dataset, test_dataset <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>random_split(total_Data, [train_size, val_size, val_size]) <span style=color:#75715e># train, val, and test splits</span>
</code></pre></div><p>For each of these datasets, a data loader was created to load a batch of images at once instead of loading the entire dataset to memory. To train the model, the training and validation dataloaders are used. This ensures that no testing data is used in any step of training the model.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>batch_sz <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span> <span style=color:#75715e># Batch size</span>
tr_dl  <span style=color:#f92672>=</span> DataLoader(train_dataset,  batch_size<span style=color:#f92672>=</span>batch_sz, shuffle<span style=color:#f92672>=</span>True,  num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># Training dataloader</span>
val_dl <span style=color:#f92672>=</span> DataLoader(val_dataset,  batch_size<span style=color:#f92672>=</span>batch_sz, shuffle<span style=color:#f92672>=</span>True,  num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)   <span style=color:#75715e># Validation dataloader</span>
test_dl <span style=color:#f92672>=</span> DataLoader(test_dataset,  batch_size<span style=color:#f92672>=</span>batch_sz, shuffle<span style=color:#f92672>=</span>True,  num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># Test dataloader</span>
</code></pre></div><h3 id=defining-the-model>Defining the model</h3><p>A simple CNN architecture was developed as a baseline model to compare performance against. Ideally, the team planned on using a more complex model to experiment with but time constraints made this unfeasable. The neural network is defined with pytorch to be the following:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
    nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,  out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>), 
    nn<span style=color:#f92672>.</span>ReLU(),
    nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>12</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
    nn<span style=color:#f92672>.</span>ReLU(),
    nn<span style=color:#f92672>.</span>ConvTranspose2d(in_channels <span style=color:#f92672>=</span> <span style=color:#ae81ff>12</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
    nn<span style=color:#f92672>.</span>ReLU(),
    nn<span style=color:#f92672>.</span>ConvTranspose2d(in_channels <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
    nn<span style=color:#f92672>.</span>Sigmoid()
)<span style=color:#f92672>.</span>cuda()
</code></pre></div><p>Using model summary, we can see the network&rsquo;s structure when given an input of shape (3,720,1280), this being the resolution of the collected colour images.</p><p><img src=images/simpleCNN.png class=center>
<em>Figure 6: A simple CNN architecture used to convert colour images to depth images.</em><div style=margin-top:3rem></div></p><p>After creating the pytorch model, a training loop was developed to train the network on the training dataset whilst validating with the validation dataset.</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/BenSnow6/BenSnow6.github.io/edit/main/content/posts/Introduction/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div></div><hr><div class="row next-prev-navigator"></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><ul><li><a href=#introduction>Introduction</a></li><li><a href=#project-proposal>Project proposal</a></li><li><a href=#approach>Approach</a></li><li><a href=#project-structure>Project structure</a></li><li><a href=#data-acquisition>Data acquisition</a></li><li><a href=#data-preprocessing>Data preprocessing</a></li><li><a href=#defining-the-model>Defining the model</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:bensnows@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span><span>bensnows@gmail.com</span></a></li><li><span><i class="fas fa-phone-alt"></i></span><span>+44 7794748089</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form method=post action=https://blogtrottr.com><div class=form-group><input type=email class=form-control name=btr_email placeholder="Enter email"><br><input type=hidden name=btr_url value=https://BenSnow6.github.ioindex.xml>
<input type=hidden name=schedule_type value=1>
<small id=emailHelp class="form-text text-muted">By entering your email address, you agree to receive the newsletter of this website.</small>
<button type=submit class="btn btn-info"> Submit</button></div></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>