<!doctype html><html><head><title>FSDL Lab 1: PyTorch</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><meta property="og:title" content="FSDL Lab 1: PyTorch"><meta property="og:description" content="Following the first set of labs from the Full Stack Deep Learning course."><meta property="og:type" content="article"><meta property="og:url" content="https://BenSnow6.github.io/posts/fullstackdeeplearning/lab-modules/lab-1/pytorch/"><meta property="article:published_time" content="2022-10-10T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-10T00:00:00+00:00"><meta name=description content="Following the first set of labs from the Full Stack Deep Learning course."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>Ben Snow's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/ title=Introduction>Introduction</a></li><li><a href=/posts/restructuring/ title=Restructuring>Restructuring</a></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/fullstackdeeplearning/>Full stack deep learning</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/fullstackdeeplearning/lab-modules/>Lab Modules</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/fullstackdeeplearning/lab-modules/lab-1/>Lab 1</a><ul class=active><li><a class=active href=/posts/fullstackdeeplearning/lab-modules/lab-1/pytorch/ title="Pytorch and setup">Pytorch and setup</a></li></ul></li></ul></li><li><a href=/posts/fullstackdeeplearning/lectures/ title=Lectures>Lectures</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/serverlessml/>Serverless ML</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/serverlessml/lab-modules/>Lab Modules</a><ul><li><a href=/posts/serverlessml/lab-modules/lab-0/ title="Lab 0">Lab 0</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/serverlessml/lab-modules/lab-1/>Lab 1</a><ul><li><a href=/posts/serverlessml/lab-modules/lab-1/end-to-end-pipeline/ title="End to End">End to End</a></li><li><a href=/posts/serverlessml/lab-modules/lab-1/refactoring/ title=Refactoring>Refactoring</a></li></ul></li></ul></li><li><a href=/posts/serverlessml/lectures/ title=Lectures>Lectures</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://BenSnow6.github.io/posts/fullstackdeeplearning/lab-modules/lab-1/pytorch/hero.svg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/avatar_hub7979d5c0b641038ae587dc7f4e229bf_336111_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Ben Snow</h5><p>:date_full</p></div><div class=title><h1>FSDL Lab 1: PyTorch</h1></div><div class=post-content id=post-content><p>10/10/2022</p><h1 id=lab-1---deep-neural-networks-in-pytorch>Lab 1 - Deep Neural Networks in PyTorch</h1><p>#machinelearning #course #fsdl</p><p>Follow along with the <a href=https://github.com/BenSnow6/FSDL_lab_1>Lab 1 notebook</a>. Or check out the course website for the <a href=https://fullstackdeeplearning.com/course/2022/labs-1-3-cnns-transformers-pytorch-lightning/>Lab 1 notes</a>.</p><h2 id=setup>Setup</h2><p>Using a bootstrap python file, the Colab environment is setup to pull the FSDL github repo and set the path variable, lab directory, hot reloading of modules, and inline plotting.</p><h3 id=downloading-data>Downloading data</h3><p>We will be using the mnist dataset of handwritten digits.
This is downloaded as a pkl.gz serialized file from the PyTorch repo with requests.
Directories are made to save the downloaded file in the Colab cloud storage.</p><p>Gzip and pickle are used to open the file and extract the contents into ((x_train, y_train), (x_valid, y_valid)). These arrays are then returned.</p><p>For us to use PyTorch, we must convert all arrays to tensors. This is done by mapping the four arrays to torch.tensors.</p><h2 id=tensors>Tensors</h2><p>Shape and dimension</p><p>The dimension of a tensor specifies how many indices you need to get a number out of an array.
e.g. 2 for a 2D tensor with rows and columns.
Use .ndim on a tensor to return its dimension</p><p>Shape tells you how many entries there are in a ONE DIMENSIONAL tensor.
Shape tells you how many rows and columns there are in a TWO DIMENSIONAL tensor.
Use .shape on a tensor to return its shape</p><h2 id=showing-example-images>Showing example images</h2><p>It is always useful to look at your data along the way. Using random.randomint and wandb, we show the label of the image and the image itself from a random index of the training data.</p><h1 id=building-a-dnn-with-torchtensor>Building a DNN with torch.Tensor</h1><p>Let&rsquo;s start simple and try to build a network that fits x_train to y_train with basic torch components.</p><p>We will start with a single layer that uses matrix multiplication and adds a vector of biases. Along with this, we will track the gradients wrt the tensors so will use &lsquo;requires_grad&rsquo; in PyTorch.</p><p>We create a set of random weights with torch.randn. The size of this weight tensor is (784, 10). The size of an input image in the training set is 784 (images are arrays of 784 entries that have already been flattened, we have 50,000 of them). The weight tensor&rsquo;s second dimension, 10, corresponds to the number of neurons we have in that layer. We wish to have 10 neurons since we have 10 classes we want to classify (0,1,2,3&mldr;9 number of handwritten digit)</p><p>We define the linear operation of multiplying an input tensor by the weight tensor and adding the bias as a function.
After this, we define a log-softmax operation to normalise the output of the model to return probabilities of each of the 10 classes. We use the log-softmax instead of regular softmax for stability reasons. Also due to its relationship to &lsquo;surprise&rsquo; of events happening. When framing the problem as an optimisation problem, we are trying to minimise the surprise of each of the outcomes. Surprise is defined as the negative log of the probability. The expectation of the surprise is the entropy of the system. Cool video here about why we use surprise and why the idea of this optimisation and surprise framing yields the KL-divergence and Maximum Likelihood Estimation. <a href="https://youtu.be/LBemXHm_Ops?t=1071">The logarithms of probabilities</a>.</p><p>Moving on, we create our model by taking the log softmax of our linear layer of the input batch.</p><h2 id=batching>Batching</h2><p>We use batches to split the training dataset up into smaller chunks. We apply our model to one batch at a time to save throwing everything at it at once.</p><p>We achieve this by use of indexing. e.g. <code>x_batch = x_train[0:batch_size]</code></p><p>The output size of the model is equal to the batch size by the dimension of the output neurons. Here the output is a tensor of shape (64, 10). and therefore a dimension of 2.</p><h2 id=loss-and-metrics>Loss and metrics</h2><p>Obviously the output of the current model is rubbish. It&rsquo;s just the log softmax of random weights multiplied by the input tensor plus a bias of zero. We need a way of quantifying how wrong this output is to inform how to improve it.</p><p><strong>Note, all logs are lns (log base e).</strong></p><p>We can start by assuming that the output from the model with the highest probability. The log softmax function is defined as $$log(\frac{exp(x_i)}{\sum_{j=1}^n exp(x_j)}) = x_j - log(\sum_{i=1}^n exp (x_i))$$ and this value is negative, so the output with the highest value has the highest predicted probability.</p><p>We now define an accuracy metric to find the difference between the highest value outputted by the model and the actual label of the input training sample. We do this by finding the argument of the highest value in the output array and comparing this to the value of the label. (The argument of the output tensor is a single number (0,1,2,3&mldr;9) and the argmax will return one of these numbers, i.e. the prediction. We compare that to the actual label, y)
The accuracy is then defined as the number of correct predictions divided by the number of total predictions for a batch of inputs.. In this model, the accuracy is around 10% (expected since there are 10 classes and we have random weight initialisation)</p><h2 id=downsides-of-using-argmax>Downsides of using argmax</h2><p>Unfortunately, the argmax function can&rsquo;t be differentiated, this is pivotal in training neural networks and in particular, they must be smoothly varying. Argmax is no longer an option for us. We can double check this by calling the .backward() function on our accuracy function and see it returns &lsquo;does not have a grad_fn&rsquo;.</p><h2 id=cross-entropy>Cross entropy</h2><p>Instead, we will use a cross-entropy function. Excellent article about <a href=https://charlesfrye.github.io/stats/2017/11/09/the-surprise-game.html>cross entropy</a> by Dr. Charles Frye can be seen here. Really recommend this blog post for understanding cross entropy for deep learning.</p><p>We use the cross entropy formula to get a new loss function that is differentiable. The expected loss of random guessing on 10 classes is going to be close to $-log(1/10)$. Which it is. 2.31 vs 2.30.</p><p>We can now call .backward() on the loss function and PyTorch will smile back at us!</p><p>The gradients are stored in the weights.grad, and bias.grad attributes of our model.</p><h2 id=training-loop>Training loop</h2><p>We now have everything we need:</p><ul><li>Data in terms of x_train and y_train</li><li>A model with weights and bias</li><li>A loss function using cross entropy that we can call .backward on to compute gradients</li></ul><p>Let&rsquo;s create a python for loop that will train the model based on the minimisation of this loss function.</p><p>We define two parameters:</p><ul><li>Learning rate (i.e. how big of a step in the downwards gradient direction we take)</li><li>Epochs (How many times the training data is put through the model)</li></ul><p>The for loop is as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span> <span style=color:#75715e># learning rate</span>
epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span> <span style=color:#75715e># numebr of training epochs</span>

<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs): <span style=color:#75715e># loop over all data</span>
	<span style=color:#66d9ef>for</span> ii <span style=color:#f92672>in</span> range((n<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>//</span> bs <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>): <span style=color:#75715e># in batches of size bs, for ~n/bs batches</span>
		start_idx <span style=color:#f92672>=</span> ii<span style=color:#f92672>*</span>bs <span style=color:#75715e># current array index</span>
		end_idx <span style=color:#f92672>=</span> start_idx <span style=color:#f92672>+</span> bs <span style=color:#75715e># end batch index</span>

		<span style=color:#75715e># Grab the x and y training samples for the current batch</span>
		xb <span style=color:#f92672>=</span> x_train[start_idx:end_idx]
		yb <span style=color:#f92672>=</span> y_train[start_idx:end_idx]

		<span style=color:#75715e># run the features through the model</span>
		pred <span style=color:#f92672>=</span> model(xb)

		<span style=color:#75715e># calculate the loss</span>
		loss <span style=color:#f92672>=</span> loss_func(pred, yb)

		<span style=color:#75715e># calculate the gradients</span>
		loss<span style=color:#f92672>.</span>backward()

		<span style=color:#75715e># update the model parameters</span>
		<span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad(): <span style=color:#75715e># don&#39;t track gradients when updating params</span>
			weights <span style=color:#f92672>-=</span> weights<span style=color:#f92672>.</span>grad <span style=color:#f92672>*</span> lr <span style=color:#75715e># update weights by learning rate*grads</span>
			bias <span style=color:#f92672>-=</span> bias<span style=color:#f92672>.</span>grad <span style=color:#f92672>*</span> lr <span style=color:#75715e># and for bias</span>
			<span style=color:#75715e># delete the current gradients or else they&#39;ll compound</span>
			weights<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>zero()
			bias<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>zero()
</code></pre></div><p>This is the very simple weight and bias update loop for training the model to minimise the loss function for the training features subject to the labels provided.</p><p>We then test this on some examples and see if the accuracy of the model has decreased (can use accuracy since this problem has no class imbalances).</p><p>Accuracy is now 100%.</p><h2 id=why-this-training-loop-is-inefficient>Why this training loop is inefficient</h2><ul><li>We have to write a custom loop every time we write a new model</li><li>Lots of hard coded assumptions</li><li>Manual tracking of hyper parameters</li><li>If we can&rsquo;t fit data into the ram then we&rsquo;re done for</li></ul><p>Let&rsquo;s look at torch.nn components to make our lives easier.</p><p>First, we don&rsquo;t need to write our own cross entropy and log softmax from scratch. PyTorch has these built in. It&rsquo;s less bug prone to use libraries that are already written.</p><p>We can find both of these in torch.nn.functional</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#f92672>as</span> F

loss_func <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>model</span>(xb):
	<span style=color:#66d9ef>return</span> xb <span style=color:#960050;background-color:#1e0010>@</span> weights <span style=color:#f92672>+</span> bias
</code></pre></div><p>We can then evaluate the model on a batch with the following:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>loss_func(model(xb), yb)
accuracy(model(xb), yb)
</code></pre></div><p>which will give us the exact same output as before.</p><p>Now, we&rsquo;re being naïve here, we are defining weights and bias outside of the model function and they are being manipulated all over the place without being tracked. We want to use weights and bias as functions e.g. when making predictions we want to pass them an input and get an output. But weights and bias are stateful (they depend on what has already happened to them and need a current state that may have been updated). They are parameterised functions and can be changed by altering their parameters (through optimisation).
We need a way of both calling these items like a function and also tracking their state like an object.
Enter <code>nn.Modlue</code></p><h1 id=nnmodule>nn.Module</h1><p>nn.Module is a PyTorch class that allows us to track state and call as a function.
For example</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MNISTLogistic</span>(nn<span style=color:#f92672>.</span>Modlue):
	<span style=color:#66d9ef>def</span> __init__(self):
		super()<span style=color:#f92672>.</span>__init__() <span style=color:#75715e># run parent class init function (nn.Modlue.__init__()</span>
		self<span style=color:#f92672>.</span>weights <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>784</span>,<span style=color:#ae81ff>10</span>)<span style=color:#f92672>/</span>math<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>784</span>))
		self<span style=color:#f92672>.</span>biad <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>10</span>))		
</code></pre></div><p>This gives us a class that we can call like a function with loss.backward() and instantiate it as an object.
We can also add a forward pass function as so:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, xb: torch<span style=color:#f92672>.</span>Tensor)<span style=color:#f92672>-&gt;</span> torch<span style=color:#f92672>.</span>Tensor:
	<span style=color:#66d9ef>return</span> xb <span style=color:#960050;background-color:#1e0010>@</span> weights <span style=color:#f92672>+</span> bias
</code></pre></div><p>and add this to the class with:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>MNISTLogistic<span style=color:#f92672>.</span>forward <span style=color:#f92672>=</span> forward
</code></pre></div><p>We can then instantiate the model as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model <span style=color:#f92672>=</span> MNISTLogistic() <span style=color:#75715e># instantiate like an object</span>
model(xb) <span style=color:#75715e># can call like a fn</span>
loss <span style=color:#f92672>=</span> loss_func(model(xb), yb) <span style=color:#75715e># use within other functions</span>
loss<span style=color:#f92672>.</span>backward() <span style=color:#75715e># can still use gradients</span>
model<span style=color:#f92672>.</span>weights<span style=color:#f92672>.</span>grad <span style=color:#75715e># grads stored in parameter&#39;s grad attribute</span>
</code></pre></div><p>Even better than all of this is the fact that we can use <code>model.parameters()</code> to iterate over all of the model parameters!
Our new training loop will look like this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>():
	<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
		<span style=color:#66d9ef>for</span> ii <span style=color:#f92672>in</span> range((n<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>//</span>bs <span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>):
			start_idx <span style=color:#f92672>=</span> ii <span style=color:#f92672>*</span> bs
			end_idx <span style=color:#f92672>=</span> start_idx <span style=color:#f92672>+</span> bs
			xb <span style=color:#f92672>=</span> x_train[start_ix:end_idx]
			yb <span style=color:#f92672>=</span> y_train[start_ix:end_idx]
			pred <span style=color:#f92672>=</span> model(xb)
			loss <span style=color:#f92672>=</span> loss_func(pred, yb)

			loss<span style=color:#f92672>.</span>backward()
			<span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
				<span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>parameters(): <span style=color:#75715e># find all model params</span>
					p <span style=color:#f92672>-=</span> p<span style=color:#f92672>.</span>grad() <span style=color:#f92672>*</span> lr <span style=color:#75715e># update parameters via SGD</span>
				model<span style=color:#f92672>.</span>zero_grad() <span style=color:#75715e># set gradients to zero to stop accumulation</span>
fit()
</code></pre></div><p>We can then calculate the accuracy as before with:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>accuracy(model(xb), yb)
</code></pre></div><p>But there&rsquo;s more!
torch.nn has so many more modules that we can use. See them all with <code>torch.nn.modules.__all__</code>.</p><p>Instead of defining our linear layer, we&rsquo;ll just use <code>nn.Linear</code> like so:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MNISTLogistic</span>(nn<span style=color:#f92672>.</span>Module):
	<span style=color:#66d9ef>def</span> __init__(self):
		super<span style=color:#f92672>.</span>__init__()
		self<span style=color:#f92672>.</span>lin <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>784</span>,<span style=color:#ae81ff>10</span>) <span style=color:#75715e># already sets up weights and biases</span>
	<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, xb):
		<span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>lin(xb) <span style=color:#75715e># cals the linear layer</span>
</code></pre></div><p>The <code>nn.Linear</code> module is a child of <code>model</code> so we can&rsquo;t directly see the weight and bias matrices, but we can access them with <code>.parameters</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model<span style=color:#f92672>.</span>children() <span style=color:#75715e># returns the Linear layer with in_features=784, out_features = 10 and bias=True</span>
model<span style=color:#f92672>.</span>parameters() <span style=color:#75715e># returns the weight and bias parameters</span>
</code></pre></div></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/BenSnow6/BenSnow6.github.io/edit/main/content/posts/FullStackDeepLearning/Lab%20Modules/Lab%201/Pytorch/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/restructuring/ title="Restructuring a project" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>Prev</div><div class=next-prev-text>Restructuring a project</div></a></div><div class="col-md-6 next-article"><a href=/posts/fullstackdeeplearning/lectures/ title=Lectures class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Lectures</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#setup>Setup</a><ul><li><a href=#downloading-data>Downloading data</a></li></ul></li><li><a href=#tensors>Tensors</a></li><li><a href=#showing-example-images>Showing example images</a></li></ul><ul><li><a href=#batching>Batching</a></li><li><a href=#loss-and-metrics>Loss and metrics</a></li><li><a href=#downsides-of-using-argmax>Downsides of using argmax</a></li><li><a href=#cross-entropy>Cross entropy</a></li><li><a href=#training-loop>Training loop</a></li><li><a href=#why-this-training-loop-is-inefficient>Why this training loop is inefficient</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://BenSnow6.github.io#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:bensnows@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span><span>bensnows@gmail.com</span></a></li><li><span><i class="fas fa-phone-alt"></i></span><span>+44 7794748089</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form method=post action=https://blogtrottr.com><div class=form-group><input type=email class=form-control name=btr_email placeholder="Enter email"><br><input type=hidden name=btr_url value=https://BenSnow6.github.ioindex.xml>
<input type=hidden name=schedule_type value=1>
<small id=emailHelp class="form-text text-muted">By entering your email address, you agree to receive the newsletter of this website.</small>
<button type=submit class="btn btn-info"> Submit</button></div></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>